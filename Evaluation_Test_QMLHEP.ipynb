{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca966414-519a-4650-8aab-f48d9a68dcc5",
   "metadata": {},
   "source": [
    "## Task I: Quantum Computing Part\n",
    "\n",
    "Note: AI was referenced when completing these tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341dbcf8-b231-4bb0-b457-ef923ee99169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ──H─╭●──────────╭SWAP──RX(1.57)─┤  State\n",
      "1: ──H─╰X─╭●───────│───────────────┤  State\n",
      "2: ──H────╰X─╭●────│───────────────┤  State\n",
      "3: ──H───────╰X─╭●─│───────────────┤  State\n",
      "4: ──H──────────╰X─╰SWAP───────────┤  State\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dev = qml.device(\"default.qubit\", wires=5)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def task1():\n",
    "    \"\"\"\n",
    "    Follows 1) implement a simple quantum operation with Cirq or Pennylane\n",
    "    \n",
    "    Returns:\n",
    "        np.array[complex]: The state of the qubit after the operations.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(5):\n",
    "        qml.Hadamard(wires=i)\n",
    "    qml.CNOT(wires=[0,1])\n",
    "    qml.CNOT(wires=[1,2])\n",
    "    qml.CNOT(wires=[2,3])\n",
    "    qml.CNOT(wires=[3,4])\n",
    "    qml.SWAP(wires=[0,4])\n",
    "    qml.RX(np.pi/2, wires=0)\n",
    "\n",
    "    return qml.state()\n",
    "\n",
    "print(qml.draw(task1)())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d049f464-b157-4107-9d4d-ad43af85d06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: ──H────────╭SWAP──────────┤  State\n",
      "1: ──RX(1.05)─│─────╭SWAP────┤  State\n",
      "2: ──H────────├SWAP─│────────┤  State\n",
      "3: ──H────────│─────├SWAP────┤  State\n",
      "4: ──H────────╰●────╰●─────H─┤  State\n"
     ]
    }
   ],
   "source": [
    "dev2 = qml.device(\"default.qubit\", wires=5)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def task2():\n",
    "    \"\"\"\n",
    "    Follows 2) Implement a second circuit with a framework of your choice:\n",
    "\n",
    "    Returns:\n",
    "        np.array[complex]: The state of the qubit after the operations.\n",
    "    \"\"\"\n",
    "\n",
    "    qml.Hadamard(wires=0)\n",
    "    qml.RX(np.pi/3, wires=1)\n",
    "    qml.Hadamard(wires=2)\n",
    "    qml.Hadamard(wires=3)\n",
    "\n",
    "    # swap test, using the fifth qubit (wires=4) as the auxiliary qubit\n",
    "    qml.Hadamard(wires=4)\n",
    "    qml.CSWAP(wires=[4, 0, 2])\n",
    "    qml.CSWAP(wires=[4, 1, 3])\n",
    "    qml.Hadamard(wires=4)\n",
    "\n",
    "    return qml.state()\n",
    "\n",
    "print(qml.draw(task2)())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5584b9ed-bc8b-4eab-934d-e62b90ea91a7",
   "metadata": {},
   "source": [
    "## Task II: Classical Graph Neural Network (GNN) \n",
    "\n",
    "For the Quark/Gluon jet classification with __[this dataset](https://zenodo.org/record/3164691#.YigdGt9MHrB)__, we will use both a GCN (graph convolutional network) and a GAN (graph attention network) as our graph-based architecture.\n",
    "\n",
    "For both architectures, we will treat particles at nodes containing its features (pt, rapidity, azimuthal angle). This makes physical sense, since jets can be thought of as made up of its constituents. We will contruct edges using k-Nearest Neighbors in ($\\eta$,$\\phi$). Graph networks are a great way to classify jets, especially since they can easily represent the energy spread of jets.\n",
    "\n",
    "Using a GCN has a lower computational cost compared to other GNNs; however, because the neighboring nodes are treated equally, it can miss certain complex particle interactions.\n",
    "\n",
    "Using a GAT adds attention to the previous approach. This increases computational cost, but allows the network to learn which nodes and features are more important. This makes physical sense, as you would expect some particles to indicate jet origin stronger than others. We also added dropout and regularization to the GAT model, as it is more complex.\n",
    "\n",
    "The GCN ended up performing better than the GAT. Since we trained on only ~80000 jets, it is very reasonable to believe that the GAT would perform better with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b6dd7a7-6008-471c-9996-e75c5e13a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import energyflow\n",
    "\n",
    "# X is 3D array of the jets (num_data,max_num_particles,4) where each jet has (pt,y,phi,pid)\n",
    "# y is labels (quark=1, gluon=0)\n",
    "X,y = energyflow.qg_jets.load(num_data=100000, pad=True, ncol=4, generator='pythia',\n",
    "                        with_bc=False, cache_dir='./')\n",
    "\n",
    "import torch\n",
    "import torch_cluster\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "# Preprocess by adding an edge index by using kNN in y,phi \n",
    "def preprocess_jet(jet, label, k=10):\n",
    "    # Remove zero-padded particles\n",
    "    mask = jet[:, 0] > 0\n",
    "    jet = jet[mask]\n",
    "\n",
    "    # Exclude pid for now\n",
    "    x = torch.tensor(jet[:, :3], dtype=torch.float)\n",
    "\n",
    "    # Construct edge index using k-NN in the y-phi space\n",
    "    edge_index = torch_cluster.knn_graph(x[:, 1:], k=k)\n",
    "\n",
    "    # Create a Data object\n",
    "    data = Data(x=x, edge_index=edge_index, y=torch.tensor([label], dtype=torch.long))\n",
    "    return data\n",
    "\n",
    "# Apply preprocessing to all jets\n",
    "data_list = [preprocess_jet(X[i], y[i]) for i in range(len(y))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "633487a3-4a41-4a18-ae75-419f7ce7af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 0.5413, Validation Accuracy: 0.7297\n",
      "Epoch: 002, Loss: 0.5308, Validation Accuracy: 0.7410\n",
      "Epoch: 003, Loss: 0.5260, Validation Accuracy: 0.7400\n",
      "Epoch: 004, Loss: 0.5229, Validation Accuracy: 0.7485\n",
      "Epoch: 005, Loss: 0.5211, Validation Accuracy: 0.7373\n",
      "Epoch: 006, Loss: 0.5205, Validation Accuracy: 0.7440\n",
      "Epoch: 007, Loss: 0.5206, Validation Accuracy: 0.7389\n",
      "Epoch: 008, Loss: 0.5197, Validation Accuracy: 0.7448\n",
      "Epoch: 009, Loss: 0.5192, Validation Accuracy: 0.7480\n",
      "Epoch: 010, Loss: 0.5202, Validation Accuracy: 0.7399\n",
      "Epoch: 011, Loss: 0.5186, Validation Accuracy: 0.7493\n",
      "Epoch: 012, Loss: 0.5185, Validation Accuracy: 0.7464\n",
      "Epoch: 013, Loss: 0.5179, Validation Accuracy: 0.7438\n",
      "Epoch: 014, Loss: 0.5174, Validation Accuracy: 0.7286\n",
      "Epoch: 015, Loss: 0.5173, Validation Accuracy: 0.7475\n",
      "Epoch: 016, Loss: 0.5178, Validation Accuracy: 0.7375\n",
      "Epoch: 017, Loss: 0.5158, Validation Accuracy: 0.7513\n",
      "Epoch: 018, Loss: 0.5155, Validation Accuracy: 0.7353\n",
      "Epoch: 019, Loss: 0.5152, Validation Accuracy: 0.7434\n",
      "Epoch: 020, Loss: 0.5172, Validation Accuracy: 0.7512\n",
      "Test Accuracy: 0.7560\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# GCN\n",
    "class GCNModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "dataset_size = len(data_list)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(data_list, [train_size, val_size, test_size])\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCNModel(in_channels=3, hidden_channels=32, num_classes=2).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = criterion(out, data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        out = model(data)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct += int((pred == data.y).sum())\n",
    "    return correct / len(loader.dataset)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "    \n",
    "test_acc = evaluate(model, test_loader, device)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb246aa7-39c6-47ee-bb44-15556682458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 002, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 003, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 004, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 005, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 006, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 007, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 008, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 009, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 010, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 011, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 012, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 013, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 014, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 015, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 016, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 017, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 018, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 019, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Epoch: 020, Loss: 2.3239, Validation Accuracy: 0.5003\n",
      "Test Accuracy: 0.4941\n"
     ]
    }
   ],
   "source": [
    "# GAT\n",
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes, heads=4):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, concat=True)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, concat=False)\n",
    "        self.lin = torch.nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "dataset_size = len(data_list)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "test_size = dataset_size - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(data_list, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model2 = GATModel(in_channels=3, hidden_channels=32, num_classes=2, heads=4).to(device)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train(model2, train_loader, optimizer, criterion, device)\n",
    "    val_acc = evaluate(model2, val_loader, device)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {train_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "    \n",
    "test_acc = evaluate(model2, test_loader, device)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618785d1-d4de-45cf-bebd-3c6984870d9d",
   "metadata": {},
   "source": [
    "## Task III: Open Task\n",
    "Quantum computing is a rapidly progressing field, with new applications being found in all kinds of disciplines. The first quantum algorithm I learned was in a cryptography class, and is probably the most infamous quantum algorithm - Shor’s algorithm. Shor’s algorithm quickly solves the factoring problem using quantum phase estimation to find the period of a function. This allows RSA encryption to be broken faster than classical algorithms, and means that RSA will be useless in a world full of quantum computers. Already, a new field of post-quantum cryptography has blossomed and it leads me to wonder what else will come from studying quantum computing. I don’t have too much experience with quantum machine learning, but I have experience with both quantum mechanics and machine learning, and I hope that this background will allow me to learn at a rapid pace. The idea of combining the Kolmogorov-Arnold representation theorem with quantum computing caught my eye as an idea with a lot of inherent potential, as one of the core ideas of quantum mechanics is superposition. I believe that a deeper study into this connection will lead to very informative results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d7511-6248-4ef5-be25-82f32891af6e",
   "metadata": {},
   "source": [
    "## Task IX: Kolmogorov-Arnold Network\n",
    "\n",
    "Below is an implementaion of a KAN using layers with learnable basis-spline activations. Since a KAN is just an MLP with spline activations, we will just use PyTorch to implement this KAN.\n",
    "\n",
    "To turn this classical KAN into a quantum KAN we would first create some quantum feature map to encode the classical data into quantum states (e.g. for MNIST, encode pixels as angles or amplitudes). As for what to use as the activation functions, more investigation would have to be done. Quantum B-splines seem like an obvious analog. These would be designed as parametrized quantum circuits that are approximations of basis-splines. Of course these would form layers in the NN where measurements of an observable determine the activation value. Parameter-shift rules will be used to optimize and update the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b56ef033-4296-4eb3-9e67-37315e850dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:00<00:00, 10.8MB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 442kB/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:00<00:00, 4.11MB/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 2.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train loss = 0.2813\n",
      "Test loss: 0.1639, Test accuracy: 0.9504\n",
      "Epoch 2: Train loss = 0.1263\n",
      "Test loss: 0.1095, Test accuracy: 0.9663\n",
      "Epoch 3: Train loss = 0.0862\n",
      "Test loss: 0.1060, Test accuracy: 0.9661\n",
      "Epoch 4: Train loss = 0.0666\n",
      "Test loss: 0.0961, Test accuracy: 0.9688\n",
      "Epoch 5: Train loss = 0.0543\n",
      "Test loss: 0.0896, Test accuracy: 0.9731\n",
      "Epoch 6: Train loss = 0.0463\n",
      "Test loss: 0.0905, Test accuracy: 0.9713\n",
      "Epoch 7: Train loss = 0.0341\n",
      "Test loss: 0.1080, Test accuracy: 0.9706\n",
      "Epoch 8: Train loss = 0.0356\n",
      "Test loss: 0.0958, Test accuracy: 0.9734\n",
      "Epoch 9: Train loss = 0.0318\n",
      "Test loss: 0.0832, Test accuracy: 0.9773\n",
      "Epoch 10: Train loss = 0.0248\n",
      "Test loss: 0.0979, Test accuracy: 0.9747\n",
      "Epoch 11: Train loss = 0.0279\n",
      "Test loss: 0.0871, Test accuracy: 0.9757\n",
      "Epoch 12: Train loss = 0.0245\n",
      "Test loss: 0.1059, Test accuracy: 0.9735\n",
      "Epoch 13: Train loss = 0.0190\n",
      "Test loss: 0.1137, Test accuracy: 0.9727\n",
      "Epoch 14: Train loss = 0.0220\n",
      "Test loss: 0.0970, Test accuracy: 0.9767\n",
      "Epoch 15: Train loss = 0.0202\n",
      "Test loss: 0.0961, Test accuracy: 0.9765\n",
      "Epoch 16: Train loss = 0.0205\n",
      "Test loss: 0.0945, Test accuracy: 0.9770\n",
      "Epoch 17: Train loss = 0.0214\n",
      "Test loss: 0.1015, Test accuracy: 0.9752\n",
      "Epoch 18: Train loss = 0.0232\n",
      "Test loss: 0.1115, Test accuracy: 0.9720\n",
      "Epoch 19: Train loss = 0.0220\n",
      "Test loss: 0.0961, Test accuracy: 0.9761\n",
      "Epoch 20: Train loss = 0.0192\n",
      "Test loss: 0.1136, Test accuracy: 0.9731\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# The learnable spline activation function, motivated by similar online work\n",
    "class LearnableSpline(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple piecewise linear spline activation.\n",
    "    Given a set of fixed knot positions and learnable coefficients,\n",
    "    for an input x, we perform linear interpolation between the two nearest knots.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_knots=10, x_min=-1.0, x_max=1.0):\n",
    "        super(LearnableSpline, self).__init__()\n",
    "        self.num_knots = num_knots\n",
    "        # Fixed knot positions (non-trainable)\n",
    "        self.register_buffer('knots', torch.linspace(x_min, x_max, num_knots))\n",
    "        # Learnable coefficients for each knot (initially set to a linear function)\n",
    "        self.coeffs = nn.Parameter(torch.linspace(x_min, x_max, num_knots))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Clamp input values to the range of knots\n",
    "        x = torch.clamp(x, self.knots[0].item(), self.knots[-1].item())\n",
    "        # Find the right interval indices for each element in x\n",
    "        indices = torch.bucketize(x, self.knots, right=False)\n",
    "        left_indices = torch.clamp(indices - 1, 0, self.num_knots - 2)\n",
    "        right_indices = left_indices + 1\n",
    "        \n",
    "        # Gather left and right knot positions and coefficients\n",
    "        left_knot = self.knots[left_indices]\n",
    "        right_knot = self.knots[right_indices]\n",
    "        left_coeff = self.coeffs[left_indices]\n",
    "        right_coeff = self.coeffs[right_indices]\n",
    "        \n",
    "        # Compute interpolation fraction\n",
    "        fraction = (x - left_knot) / (right_knot - left_knot + 1e-8)\n",
    "        # Linear interpolation between left and right coefficients\n",
    "        y = left_coeff + fraction * (right_coeff - left_coeff)\n",
    "        return y\n",
    "\n",
    "# KAN\n",
    "class KAN_MNIST(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim1=256, hidden_dim2=128, output_dim=10, num_knots=10):\n",
    "        super(KAN_MNIST, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
    "        self.spline1 = LearnableSpline(num_knots=num_knots, x_min=-3.0, x_max=3.0)\n",
    "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.spline2 = LearnableSpline(num_knots=num_knots, x_min=-3.0, x_max=3.0)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.spline1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.spline2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.size(0)\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}: Train loss = {avg_loss:.4f}\")\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            total_loss += criterion(output, target).item() * data.size(0)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "    avg_loss = total_loss / len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    print(f\"Test loss: {avg_loss:.4f}, Test accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = KAN_MNIST().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, criterion, epoch)\n",
    "    test(model, device, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
